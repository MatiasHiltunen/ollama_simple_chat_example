# Ollama Local AI-Chat example

### Prerequisities

1. Ollama needs to be installed https://ollama.com/ and a model small enough for the device to be able to run it needs to be loaded.
2. Run the model and expose ollama API to network (or use cli serve)

### Getting started

1. Run `npm install` command in project's root
2. Run `npm run dev` to start the dev-server


